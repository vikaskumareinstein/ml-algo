<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>Using K-Nearest Neighbors</title>

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/mywork/css/main.css">
	<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,900,400italic%7CSignika:700,300,400,600' rel='stylesheet' type='text/css'>

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

</head>


<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/mywork/">Home</a>

		<!-- Nav pages -->
	  
	    
	  
	    
	      <a href="/mywork//about" title="About Me">About Me</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
    
    <!-- Nav links -->
	  <a href="https://github.com/KingFelix/emerald/archive/master.zip">Download</a>
<a href="https://github.com/KingFelix/emerald">Project on Github</a>

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.1.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header">
	<a href="/mywork/">
		<img src="/mywork/img/emerald.png" alt="Emerald Logo">
	  <h1>Machine Learning and Artificial Intelligence</h1>
	</a>
</header>

      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>Using K-Nearest Neighbors</h2>		
	<time datetime="2019-10-25T00:00:00+05:30" class="by-line">25 Oct 2019</time>
	<div class="content">

		<p>In this blog we are going to investigate about a Supervised Machine Learning algorithm:K-Nearest Neighbors.This algorithm is basically based on the fact that similar things happen near their own kind.We will be measuring closeness of the given data-point with the other available classes.The “K” (in k-nearest neighbor) denotes the number of neighboring data points to be taken under consideration;if classification,that class having the larger no. of data points wins.<figure></figure></p>

<p>The algorithm for the K-nearest neighbour:
 1.Load the data and initialise the value of k
 2.Iterate from 1 to total no. of data points in training dataset
 3.Calculate the distance between the test data and each training data using euclidean distance metric.Other metrics of distance measurement includes Manhattan,Minkowski,Hamming,Chevshev ,etc.
 4.Sort the calculated distance in ascending order and enlist top k rows.
 5.The most frequent occuring class will be your answer.</p>

<p>Also,we are going to implement the code in python:-
 <code class="highlighter-rouge">import math
  from collections import Counter
   def knn(data,)</code></p>

		
	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer class="footer"><span>@2015 - Emerald</span></footer>

	    <!-- Script -->
      <script src="/mywork/js/main.js"></script>	


	</div>
</body>
</html>
